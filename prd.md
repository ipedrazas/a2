# A2 - Product Requirements Document

## Document Information

| Field | Value |
|-------|-------|
| **Product Name** | A2 (Application Analysis Tool) |
| **Version** | 1.0 |
| **Last Updated** | 2025-01-30 |
| **Status** | Active Development |

---

## Executive Summary

A2 is a code quality checker that runs configurable checks against repositories and provides actionable health/maturity scores. It addresses the growing challenge of maintaining code quality standards in projects developed with AI assistance, ensuring that code meets baseline expectations regardless of how it was created.

### Key Value Propositions

1. **AI-Generated Code Quality Assurance**: Specifically designed to validate and improve "AI slop" - code generated by AI tools that may lack proper testing, documentation, or best practices
2. **Multi-Language Support**: Comprehensive checks for 8 programming languages (Go, Python, Node.js, TypeScript, Java, Rust, Swift, and language-agnostic checks)
3. **Context-Aware Analysis**: Application profiles (CLI, API, Library, Desktop) automatically skip irrelevant checks
4. **Maturity-Based Evaluation**: Adjusts expectations based on project stage (PoC, Development, Mature, Production-Ready)
5. **AI-Optimized Output**: TOON format designed for efficient consumption by coding agents

---

## Problem Statement

### Primary Problems

1. **Inconsistent Code Quality**: AI-generated code often lacks proper testing, documentation, error handling, and security considerations
2. **No Baseline Standards**: Teams lack clear, measurable criteria for what constitutes "production-ready" code
3. **Manual Review Overhead**: Code reviews are time-consuming and subjective, especially for AI-generated contributions
4. **Tool Fragmentation**: Different ecosystems have different quality tools with inconsistent interfaces
5. **CI/CD Integration Gaps**: Existing quality gates are often complex to set up and maintain

### Target Users

- **Engineering Teams**: Maintaining quality standards across multiple projects
- **Open Source Maintainers**: Validating contributions from automated tools
- **AI-Assisted Developers**: Ensuring AI-generated code meets production standards
- **DevOps Engineers**: Integrating quality gates into CI/CD pipelines
- **Engineering Managers**: Establishing measurable quality metrics

---

## Product Vision

To become the universal standard for automated code quality assessment, providing consistent, actionable feedback across all programming languages and project types.

### Success Metrics

- **Adoption**: Number of repositories using A2 in CI/CD
- **Impact**: Reduction in production bugs attributable to code quality issues
- **Efficiency**: Time saved in code review process
- **Developer Satisfaction**: Positive feedback on actionability of recommendations

---

## Core Features

### 1. Language Detection and Analysis

**Description**: Automatically detect programming languages from project files and run appropriate checks.

**Supported Languages**:
- Go (go.mod/go.sum detection)
- Python (pyproject.toml, setup.py, requirements.txt detection)
- Node.js (package.json + lock files detection)
- TypeScript (tsconfig.json detection)
- Java (pom.xml, build.gradle detection)
- Rust (Cargo.toml detection)
- Swift (Package.swift detection)

**Requirements**:
- Support multi-language projects
- Allow manual language override via configuration
- Provide clear feedback on detected languages

---

### 2. Comprehensive Check Suite

**Description**: 80+ automated checks across multiple quality dimensions.

#### Language-Specific Checks (per language)

**Critical Checks** (must pass for production readiness):
- Project structure verification (module/package files)
- Build compilation (`go build`, `npm run build`, etc.)
- Test suite execution
- Race condition detection (Go)

**Quality Checks** (recommendations for improvement):
- Code formatting (gofmt, black, prettier, rustfmt, etc.)
- Linting (vet, flake8, ESLint, clippy, etc.)
- Static type checking (mypy, tsc, etc.)
- Test coverage measurement (configurable thresholds)
- Dependency vulnerability scanning (govulncheck, npm audit, etc.)
- Cyclomatic complexity analysis
- Structured logging verification

#### Common (Language-Agnostic) Checks

**Infrastructure**:
- Dockerfile validation with security scanning (Trivy)
- CI/CD pipeline detection
- Kubernetes manifests validation

**Documentation**:
- README.md existence
- LICENSE file verification
- API documentation (OpenAPI/Swagger)
- Contributing guidelines (CONTRIBUTING.md)
- Changelog maintenance (CHANGELOG.md)

**Security**:
- Secret scanning (gitleaks)
- Static Application Security Testing (semgrep)
- License compliance checking
- Environment configuration (.env.example)

**Operational Readiness**:
- Health endpoint implementation
- Metrics/observability instrumentation
- Error tracking integration
- Distributed tracing support
- Graceful shutdown handling
- Database migration support
- Configuration validation
- Retry logic implementation

**Testing**:
- Integration test detection
- End-to-end test detection

**Development Experience**:
- Pre-commit hooks
- Editor configuration (.editorconfig)

---

### 3. Application Profiles

**Description**: Context-aware check selection based on application type.

#### Built-in Profiles

**CLI Profile**:
- Disables: health endpoints, k8s, metrics, API docs, integration tests, graceful shutdown, error tracking, E2E tests, distributed tracing
- Focus: Command-line tool quality and user experience

**API Profile**:
- Disables: E2E tests (uses integration tests instead)
- Focus: Web service/API with full operational readiness

**Library Profile**:
- Disables: containerization, health endpoints, deployment, metrics, error tracking, integration tests, distributed tracing, E2E tests, API docs
- Focus: Code quality for reusable packages and SDKs

**Desktop Profile**:
- Disables: health endpoints, k8s, API docs, tracing, metrics, graceful shutdown, Dockerfile
- Focus: User-facing desktop application quality

**Requirements**:
- Users must be able to create custom profiles
- Profiles must be composable (extend built-in profiles)
- Profile source tracking (built-in vs user-defined)

---

### 4. Maturity Assessment

**Description**: Progressive evaluation system that adjusts expectations based on project stage.

#### Maturity Levels

**Production-Ready** (100% score, no failures):
- All checks pass
- Ready for production deployment
- No warnings

**Mature** (≥80% score, no failures):
- Most checks pass
- Minor improvements recommended
- Warnings present but no failures

**Development** (≥60% score, ≤2 failures):
- Core functionality works
- Quality improvements needed
- Limited failures acceptable

**Proof of Concept** (<60% score or >2 failures):
- Early stage development
- Focus on core functionality
- Multiple quality gaps

#### Maturity Targets (Configuration)

**Production Target**:
- All checks enabled
- Strict coverage thresholds
- Full security scanning
- Operational readiness required

**PoC Target**:
- Minimal checks (build, project structure)
- Skips: license, coverage, security scans
- Focus on core functionality

**Requirements**:
- Scoring must exclude Info status (optional tools)
- Critical failures immediately cap maturity level
- Provide actionable recommendations for next level
- Clear progression path between levels

---

### 5. Output Formats

**Description**: Multiple output formats optimized for different use cases.

#### Pretty Terminal Output
- Human-readable with color coding
- Progress indicators during execution
- Detailed check results with suggestions
- Summary statistics and maturity assessment

#### JSON Output
- Machine-readable structured data
- Complete metadata and results
- CI/CD integration friendly
- Optional raw output inclusion

#### TOON (Token-Oriented Object Notation)
- **Purpose**: Optimized for coding agents
- **Features**:
  - Minimal token format
  - Tabular results array
  - Smart quoting and compact encoding
  - Verbosity levels (normal, failures, all)
- **Structure**:
  ```
  results[N]{name,id,passed,status,message,language,duration_ms,raw_output}:
  ```

**Requirements**:
- All formats must include same core data
- Consistent exit codes (0=Pass, 1=Warn, 2+=Fail)
- Verbosity flags for detail level control

---

### 6. Configuration System

**Description**: Flexible YAML-based configuration for all aspects of checking.

#### Configuration File (`.a2.yaml`)

**Language Configuration**:
```yaml
language:
  explicit: ["go"]           # Override auto-detection
  go:
    coverage_threshold: 80    # Custom thresholds
    cyclomatic_threshold: 15
```

**Required Files**:
```yaml
files:
  required:
    - README.md
    - LICENSE
    - CONTRIBUTING.md
```

**Check Filtering**:
```yaml
checks:
  disabled:
    - "*:tests"              # Wildcard support
    - "node:*"               # All Node.js checks
    - "go:deps"              # Specific check
```

**Execution Options**:
```yaml
execution:
  parallel: true             # Parallel execution
  timeout: 300               # Per-check timeout
```

**External Checks**:
```yaml
external:
  - id: custom-lint
    name: Custom Lint Check
    command: golangci-lint
    args: ["run", "./..."]
    severity: warn           # or "fail"
```

**Requirements**:
- Interactive configuration generator (`a2 add -i`)
- Non-interactive mode (`a2 add --profile=api`)
- Profile-based defaults
- Configuration validation

---

### 7. External Checks

**Description**: Support for custom quality tools through simple protocol.

**Protocol**:
- Exit code 0: Pass
- Exit code 1: Warning
- Exit code 2+: Failure

**Features**:
- Custom command execution
- Configurable severity level
- Argument passing
- Full output capture
- Ordered execution with built-in checks

**Requirements**:
- Simple integration (no complex APIs)
- Works with any command-line tool
- Clear output in all formats

---

### 8. Wildcard Support

**Description**: Flexible pattern matching for check filtering.

**Patterns**:
- `*:tests` - All test checks across languages
- `go:*` - All Go checks
- `*:logging` - All logging checks
- `*:*` - All checks (use with caution)

**Requirements**:
- Must work in `checks.disabled` list
- Backward compatible with exact match
- Clear error messages for invalid patterns

---

## User Interface

### CLI Interface

**Primary Command**:
```bash
a2 check [flags]           # Run all checks
a2 check --profile=api     # Use application profile
a2 check --target=poc      # Use maturity target
```

**Single Check Execution**:
```bash
a2 run go:race             # Run specific check with full output
a2 explain go:build        # Get check details
```

**Configuration**:
```bash
a2 add -i                  # Interactive config generator
a2 add --profile=library   # Non-interactive mode
```

**Output Formats**:
```bash
a2 check --output=json     # JSON format
a2 check --output=toon     # Agent-optimized format
a2 check --verbose         # Detailed output
```

**Flags**:
- `--dir`: Specify source directory
- `--format`: Output format (pretty, json, toon)
- `--profile`: Application profile
- `--target`: Maturity target
- `--timeout`: Global timeout
- `--parallel`: Enable/disable parallel execution

### Server Mode (HTTP API)

**Endpoints**:
- `POST /api/analyze`: Submit GitHub URL for analysis
- `GET /api/jobs/{id}`: Get job status
- `GET /api/jobs/{id}/results`: Get analysis results

**Features**:
- Real-time job status updates
- Web UI for job submission
- Results filtering and export
- GitHub URL integration

---

## Technical Architecture

### Core Components

1. **CLI Layer** (`cmd/root.go`): Cobra-based command parsing and execution
2. **Language Detection** (`pkg/language/`): Auto-detection from indicator files
3. **Check Registry** (`pkg/checks/registry.go`): Centralized check management
4. **Runner** (`pkg/runner/`): Parallel/sequential execution with timeout handling
5. **Output Formatters** (`pkg/output/`): Format-specific rendering
6. **Configuration** (`pkg/config/`): YAML parsing and validation

### Check Implementation Pattern

All checks implement the `Checker` interface:
```go
type Checker interface {
    ID() string           // Unique identifier (e.g., "go:build")
    Name() string         // Human-readable name
    Run(path string) (Result, error)
}

type Result struct {
    Status     Status      // Pass, Warn, Fail, Info
    Message    string      // Human-readable message
    Suggestion string      // Actionable recommendation
    RawOutput  string      // Tool output
    Duration   time.Duration
}
```

### Technology Stack

- **Language**: Go 1.21+
- **CLI Framework**: Cobra
- **Configuration**: YAML
- **Testing**: Testify suite pattern
- **Build Tool**: Task (Taskfile)

---

## Non-Functional Requirements

### Performance
- Complete check suite execution in < 5 minutes for typical projects
- Parallel execution by default
- Configurable per-check timeouts
- Efficient language detection

### Reliability
- Graceful handling of missing tools (Info status)
- Clear error messages for configuration issues
- No false negatives (critical issues must be detected)

### Extensibility
- Simple interface for adding new checks
- Language package structure
- External check support for custom tools
- User-defined profiles

### Usability
- Clear, actionable error messages
- Helpful suggestions for improvement
- Interactive configuration mode
- Comprehensive documentation

### Compatibility
- macOS, Linux, Windows support
- Works with monorepos
- Compatible with common CI/CD systems
- No external dependencies for basic operation

---

## Integration Requirements

### CI/CD Integration

**GitHub Actions**:
```yaml
- name: Run A2 checks
  uses: ipedrazas/a2-action@v1
  with:
    profile: api
    target: production
```

**Pre-commit Hooks**:
```yaml
- repo: https://github.com/ipedrazas/a2
  hooks:
    - id: a2
      args: [--profile=library]
```

### Docker Support
- Containerized execution
- Volume mounting for source code
- Configurable via environment variables

---

## Security Considerations

### Tool Execution Safety
- Timeout protection for all external commands
- Input validation for GitHub URLs
- No arbitrary code execution in server mode

### Dependency Scanning
- Integration with language-specific vulnerability scanners
- Automatic security check updates

### Secret Detection
- Built-in secret scanning (gitleaks)
- Configurable patterns
- CI/CD integration

---

## Testing Strategy

### Unit Tests
- Test suite pattern (testify/suite)
- Mock external command execution
- Configuration validation tests
- Check logic isolation

### Integration Tests
- Real tool execution (requires tools installed)
- End-to-end workflow tests
- Multi-language project tests

### Test Coverage
- Target: >70% code coverage
- Critical paths: >90% coverage

---

## Documentation Requirements

### User Documentation
- Getting started guide
- Configuration reference
- Check descriptions and rationale
- Troubleshooting guide
- Output format documentation

### Developer Documentation
- Architecture overview
- Check implementation guide
- Contribution guidelines
- API documentation (server mode)

### Examples
- Sample configurations for each profile
- CI/CD integration examples
- Custom check examples

---

## Future Roadmap

### Phase 1: Core Completion (Current)
- Complete all language implementations
- Comprehensive test coverage
- Documentation

### Phase 2: Enhanced Features
- Custom check marketplace
- Historical tracking and trend analysis
- Team dashboards
- Performance benchmarking

### Phase 3: Ecosystem
- IDE extensions (VS Code, JetBrains)
- Git provider integrations (GitLab, Bitbucket)
- Self-hosted enterprise version
- Custom policy engine

### Phase 4: AI Integration
- Automated fix suggestions
- PR review integration
- Learning from team patterns
- Predictive quality metrics

---

## Success Criteria

### Product Metrics
- 1,000+ GitHub stars
- 10,000+ weekly downloads
- 500+ repositories using A2 in CI/CD
- Average project sees 30% reduction in quality issues

### Technical Metrics
- <5% false positive rate
- <5 minute execution time for 90% of projects
- 99.9% uptime for server mode
- 100+ available checks

---

## Open Questions

1. **Historical Data**: Should A2 track results over time to show trends?
2. **Fix Automation**: Should A2 automatically fix certain issues (e.g., formatting)?
3. **Team Policies**: How should enterprise teams define and enforce custom quality standards?
4. **Performance**: How to optimize for very large monorepos?
5. **Integration**: Should A2 provide deeper IDE integration beyond CLI?

---

## Appendix

### A. Current Check Catalog

Full list of 80+ checks available in `pkg/checks/` directory.

### B. Exit Codes

- 0: Pass (all checks passed)
- 1: Warning (warnings present, no failures)
- 2+: Failure (one or more checks failed)

### C. File Extensions and Indicators

Language detection indicators documented in `pkg/language/detector.go`.

### D. Glossary

- **Check**: Single quality verification (e.g., "go:build")
- **Profile**: Pre-configured set of disabled checks for an application type
- **Target**: Maturity level controlling check strictness
- **Status**: Result of a check (Pass, Warn, Fail, Info)
- **TOON**: Token-Oriented Object Notation, agent-optimized output format
